{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63401a72-d8de-4547-8d3a-371ea323e273",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## INTRO TO MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f8c54fe-7f42-4e87-beb1-faccfb1e7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "melb_file_path = '/home/vahid/Documents/ML_Learning/melb_data.csv'\n",
    "melb_data = pd.read_csv(melb_file_path)\n",
    "melb_data.describe()\n",
    "list(melb_data.columns)\n",
    "filtered_melb_data = melb_data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb6481e-a447-443c-9467-ede13aa4a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction for the following 5 houses:\n",
      "   Rooms  Bathroom  Landsize  BuildingArea  YearBuilt  Lattitude  Longtitude\n",
      "1      2       1.0     156.0          79.0     1900.0   -37.8079    144.9934\n",
      "2      3       2.0     134.0         150.0     1900.0   -37.8093    144.9944\n",
      "4      4       1.0     120.0         142.0     2014.0   -37.8072    144.9941\n",
      "6      3       2.0     245.0         210.0     1910.0   -37.8024    144.9993\n",
      "7      2       1.0     256.0         107.0     1890.0   -37.8060    144.9954\n",
      "\n",
      "the prediction are: \n",
      "[1035000. 1465000. 1600000. 1876000. 1636000.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "434.71594577146544"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y = filtered_melb_data.Price\n",
    "melb_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_melb_data[melb_features]\n",
    "\n",
    "melb_model = DecisionTreeRegressor(random_state=1)\n",
    "melb_model.fit(X,y)\n",
    "\n",
    "print(\"Making prediction for the following 5 houses:\")\n",
    "print(X.head())\n",
    "print(\"\\nthe prediction are: \")\n",
    "print(melb_model.predict(X.head()))\n",
    "\n",
    "predicted_home_prices = melb_model.predict(X)\n",
    "mean_absolute_error(y, predicted_home_prices)\n",
    "\n",
    "# this kind of modeling has a problem. because we've used the same data \n",
    "# for fitting our model and evaluating our model. \n",
    "# we need to split our datas to two different part, one for fitting or \n",
    "# trainnig data and another part for evaluating how good our model works. \n",
    "# in the next cell we go through that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91368b17-bc77-4ceb-962d-d1b1180a886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262494.3027759845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y =train_test_split(X, y, random_state= 0)\n",
    "melb_model.fit(train_X, train_y)\n",
    "\n",
    "val_predictions = melb_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))\n",
    "\n",
    "# we splitted our data and we can see how much our mae changed. \n",
    "# it's true that the last one has really low mae but it is not true\n",
    "# this one say us more trues about how our model is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2308c879-8172-4506-90b2-11c069993f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[347380.33833344496,\n",
       " 258171.21202406782,\n",
       " 243495.96361790417,\n",
       " 254983.64299548094]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overfitting and underfitting: \n",
    "# these two terms related to how deep our tree is. \n",
    "# and the deepest tree has more leaf, so we need to choose the best \n",
    "# number for the leaves: \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y,preds_val)\n",
    "    return(mae)\n",
    "\n",
    "my_mae = list()\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae.append(get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4195f256-8962-4330-9836-25e016808777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191669.7536453626\n"
     ]
    }
   ],
   "source": [
    "# Randon Forest: \n",
    "# in decission tree we use only one tree and that tree may be over \n",
    "# or under fitted. but here we use a lot of tree and make prediction \n",
    "# based on that. \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355728f-2b01-4a79-bcf3-fcb9d103d6db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intermediate Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d87041-95b7-4893-8d5e-a28d4fcf864c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>11694</td>\n",
       "      <td>2007</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>6600</td>\n",
       "      <td>1962</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>13360</td>\n",
       "      <td>1921</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>13265</td>\n",
       "      <td>2002</td>\n",
       "      <td>1689</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>13704</td>\n",
       "      <td>2001</td>\n",
       "      <td>1541</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
       "Id                                                                    \n",
       "619    11694       2007      1828         0         2             3   \n",
       "871     6600       1962       894         0         1             2   \n",
       "93     13360       1921       964         0         1             2   \n",
       "818    13265       2002      1689         0         2             3   \n",
       "303    13704       2001      1541         0         2             3   \n",
       "\n",
       "     TotRmsAbvGrd  \n",
       "Id                 \n",
       "619             9  \n",
       "871             5  \n",
       "93              5  \n",
       "818             7  \n",
       "303             6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data \n",
    "#and defineing trainning and val and test data: \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# reading the data: \n",
    "file_path = '/home/vahid/Documents/ML_Learning/train.csv'\n",
    "X_full = pd.read_csv(file_path, index_col='Id')\n",
    "X_test_full = pd.read_csv(file_path, index_col='Id')\n",
    "\n",
    "# obtain target(dependent variable) and predictors(independent variables):\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647a63b4-ceb9-4ccf-9caf-76ec03a635bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 24015\n",
      "Model 2 MAE: 23740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:403: FutureWarning: Criterion 'mae' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='absolute_error'` which is equivalent.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 MAE: 23528\n",
      "Model 4 MAE: 23996\n",
      "Model 5 MAE: 23706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:403: FutureWarning: Criterion 'mae' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='absolute_error'` which is equivalent.\n",
      "  warn(\n",
      "/home/vahid/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:403: FutureWarning: Criterion 'mae' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='absolute_error'` which is equivalent.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for fitting model with the whole data =  0.9332040782440739\n",
      "R2 for fitting model with training data =  0.9332040782440739\n"
     ]
    }
   ],
   "source": [
    "# here we use randomForest model\n",
    "# to has prediction about future datas. \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# we defining several model of RandomForest with different parameters\n",
    "# to find the best sets of parameters: \n",
    "\n",
    "# like decission tree, here we may face over and under fitting\n",
    "# so we change our parameters to find the best. \n",
    "# n_stimators give us the number of tree in the forest. \n",
    "\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different models\n",
    "# notice: here in the parameters of the function, we put special \n",
    "# amount of data to each of the X_t, X_v , ... \n",
    "# so, in the future for calling the function we dont need to define \n",
    "# these values, we only call function with parameters of model. \n",
    "\n",
    "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))\n",
    "    \n",
    "    \n",
    "# so, untill now we find the best parameters for randomforest model. \n",
    "# now, we have the best model to predict the future data. \n",
    "# now, we fit our model with the whole data, X, y \n",
    "# here I have a question??? do we need fit our model with the whole data? \n",
    "# ????????????????????????????????????????\n",
    "# in my_model2 I fit the model with training data.\n",
    "#it give the same results. \n",
    "\n",
    "my_model = model_3\n",
    "my_model.fit(X, y)\n",
    "\n",
    "my_model2 = model_3\n",
    "my_model2.fit(X_train, y_train)\n",
    "\n",
    "# Generate test predictions\n",
    "preds_test = my_model.predict(X_test)\n",
    "preds_test2 = my_model2.predict(X_test)\n",
    "\n",
    "# finding out how good our model is by calculating R2\n",
    "# but I have the previous problem yet, \n",
    "# is it correct to fit model with the whole data???????????\n",
    "# ?????????????????????????????????????????????????????????\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y, preds_test)\n",
    "r22 = r2_score(y, preds_test2)\n",
    "\n",
    "print('R2 for fitting model with the whole data = ',r2)\n",
    "print('R2 for fitting model with training data = ',r22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11995854-1602-4c0a-a60d-1ca85dcafaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns with missing values:  ['Car', 'BuildingArea', 'YearBuilt']\n",
      "\n",
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "183550.22137772635\n",
      "\n",
      "MAE from Approach 2 (Imputation):\n",
      "178166.46269899711\n",
      "\n",
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "178927.503183954\n"
     ]
    }
   ],
   "source": [
    "## Missing values: \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the data\n",
    "file_path = '/home/vahid/Documents/ML_Learning/melb_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Function for comparing different approaches;\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "\n",
    "## Approach 1 (Drop Columns with Missing Values):\n",
    "\n",
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "print('columns with missing values: ',cols_with_missing) \n",
    "\n",
    "# dropping columns in training and validation data: \n",
    "# be carefull to drop columns in both training and validation data: \n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"\\nMAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "\n",
    "## Approach 2 (Imputation):\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation:\n",
    "# we use SimpleImputer to replace missing values..\n",
    "# with the mean value along each column.\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "# SimpleImputer() give us result in numpy.ndarray\n",
    "# so we use pd.DataFrame to have pandas format of datas. \n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "# now we need transform, because we've fitted our data in previous step.\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"\\nMAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "\n",
    "## Approach 3 (An Extension to Imputation):\n",
    "# we impute the missing values,...\n",
    "# while also keeping track of which values were imputed.\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation:\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"\\nMAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1654e68c-2fec-46c7-909a-809c5e76331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Type', 'Method', 'Regionname']\n",
      "\n",
      "MAE from Approach 1 (Drop categorical variables):\n",
      "175703.48185157913\n",
      "\n",
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "165936.40548390493\n",
      "MAE from Approach 3 (One-Hot Encoding):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166089.4893009678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahid/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Categorical Variables: \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## preaparing data: \n",
    "# Load the data\n",
    "file_path = '/home/vahid/Documents/ML_Learning/melb_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique()<10 and X_train_full[cname].dtype==\"object\"]\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes =='object')\n",
    "object_cols = list(s[s].index)\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "\n",
    "## Three Aproach of dealing with Categorical Variables: \n",
    "\n",
    "#Approach 1 (Drop Categorical Variables):\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "print(\"\\nMAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "#Approach 2 (Ordinal Encoding):\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "# Apply ordinal encoder to each column with categorical data:\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "print(\"\\nMAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "#Approach 3 (One-Hot Encoding):\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0fc0c6d-c0e6-4fe2-aa1e-8102f43f4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 160679.18917034855\n"
     ]
    }
   ],
   "source": [
    "## Pipelines:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## preaparing data: \n",
    "# Load the data\n",
    "file_path = '/home/vahid/Documents/ML_Learning/melb_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "## Step 1: Define Preprocessing Steps:\n",
    "# Preprocessing for numerical data: \n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "# Preprocessing for categorical data: \n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "# Bundle preprocessing for numerical and categorical data:\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),('cat', categorical_transformer, categorical_cols)])\n",
    "## Step 2: Define the Model: \n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "## Step 3: Create and Evaluate the Pipeline: \n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572d6b2d-47b5-4854-bd2f-b6957c558880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [301628.7893587  303164.4782723  287298.331666   236061.84754543\n",
      " 260383.45111427]\n",
      "\n",
      "Average MAE score (across experiments):\n",
      "277707.3795913405\n"
     ]
    }
   ],
   "source": [
    "## Cross-Validation:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## preaparing data: \n",
    "# Load the data\n",
    "file_path = '/home/vahid/Documents/ML_Learning/melb_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# defining a pipeline for preprocessing and modeling: \n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()), ('model', RandomForestRegressor(n_estimators=50, random_state=0))])\n",
    "\n",
    "# cross-validation process: \n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(\"MAE scores:\\n\", scores)\n",
    "### NOTICE: \n",
    "# Scikit-learn has a convention where all metrics are defined...\n",
    "# so a high number is better. Using negatives here allows them...\n",
    "# to be consistent with that convention,...\n",
    "# though negative MAE is almost unheard of elsewhere....\n",
    "\n",
    "# we want a single measure of model quality so: \n",
    "print(\"\\nAverage MAE score (across experiments):\")\n",
    "print(scores.mean())\n",
    "\n",
    "### IMPORTANT NOTICE: \n",
    "# Using cross-validation yields a much better measure of model quality,\n",
    "# with the added benefit of cleaning up our code:\n",
    "## note that we no longer need to keep track of separate\n",
    "# training and validation sets.\n",
    "# So, especially for small datasets, it's a good improvement!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc928762-146f-40a0-b930-ad0474642e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/vahid/.local/lib/python3.10/site-packages/xgboost/sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 234738.34581645802\n"
     ]
    }
   ],
   "source": [
    "## XGBoost: \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## preaparing data: \n",
    "# Load the data\n",
    "file_path = '/home/vahid/Documents/ML_Learning/melb_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "# Select target\n",
    "y = data.Price\n",
    "# Separate data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n",
    "\n",
    "##XGBoost model: \n",
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50fb3c8-652d-4041-ba0a-d413ee9bfe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 1319\n",
      "Cross-validation accuracy: 0.980294\n",
      "Fraction of those who did not receive a card and had no expenditures: 1.00\n",
      "Fraction of those who received a card and had no expenditures: 0.02\n",
      "Cross-val accuracy: 0.832446\n"
     ]
    }
   ],
   "source": [
    "## Data Leakage:\n",
    "# There are two main types of leakage:\n",
    "# target leakage and train-test contamination.\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Read the data\n",
    "data_path = '/home/vahid/Documents/ML_Learning/AER_credit_card_data.csv'\n",
    "data = pd.read_csv(data_path, true_values=['yes'], false_values=['no'])\n",
    "# Select target\n",
    "y = data.card\n",
    "# Select predictors\n",
    "X = data.drop(['card'], axis=1)\n",
    "print(\"Number of rows in the dataset:\", X.shape[0])\n",
    "X.head()\n",
    "\n",
    "# Since there is no preprocessing, we don't need a pipeline\n",
    "# (used anyway as best practice!)\n",
    "my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\n",
    "cv_scores = cross_val_score(my_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation accuracy: %f\" % cv_scores.mean())\n",
    "\n",
    "expenditures_cardholders = X.expenditure[y]\n",
    "expenditures_noncardholders = X.expenditure[~y]\n",
    "\n",
    "print('Fraction of those who did not receive a card and had no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))\n",
    "print('Fraction of those who received a card and had no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))\n",
    "\n",
    "# Drop leaky predictors from dataset\n",
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "\n",
    "# Evaluate the model with leaky predictors removed\n",
    "cv_scores = cross_val_score(my_pipeline, X2, y, \n",
    "                            cv=5,\n",
    "                            scoring='accuracy')\n",
    "\n",
    "print(\"Cross-val accuracy: %f\" % cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56fa778-efa5-4c89-8958-d1cd5ddaaff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
